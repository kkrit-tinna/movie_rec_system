{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11361713,"sourceType":"datasetVersion","datasetId":7111139},{"sourceId":12087530,"sourceType":"datasetVersion","datasetId":3816617}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shikristin/top-n-movie-recommendation-using-cosine-similarity?scriptVersionId=244333172\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Movie Recommendation System","metadata":{}},{"cell_type":"code","source":"# there is compatbility issue with gensim & numpy, run the following command to upgrade gensim if problem persists\n\n# %pip install --upgrade numpy==1.26.0\n# %pip install --upgrade gensim==4.3.3\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import StandardScaler\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport string\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('omw-1.4')\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom scipy.sparse import hstack, csr_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:10:02.207692Z","iopub.execute_input":"2025-04-11T03:10:02.207934Z","iopub.status.idle":"2025-04-11T03:10:06.375411Z","shell.execute_reply.started":"2025-04-11T03:10:02.207911Z","shell.execute_reply":"2025-04-11T03:10:06.373939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The main objective of this project is to create **a movie recommendation system** based on the TMDb dataset(The Movie Database), which is a comprehensive movie database that provides information about movies, including details like titles, ratings, release dates, revenue, genres, and much more.\n<br><br>The original dataset can be found here https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies/data\n<br>It contains real-time movie information and updated to include movies in 2024.","metadata":{}},{"cell_type":"markdown","source":"## Model Development","metadata":{}},{"cell_type":"markdown","source":"This recommendation system suggests similar movies based on the ***similarity*** between each movie, based on genres, keywords, ratings, language etc.","metadata":{}},{"cell_type":"markdown","source":"### Data Exploration","metadata":{}},{"cell_type":"code","source":"# load dataset\ndf = pd.read_csv('TMDB_movie_dataset_v11.csv')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# display all columns & the head rows of the dataset\npd.set_option('display.max_columns', None)\ndf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are over 1 million records of movies in our dataset, 23 attributes are documented, including title, vote counts, revenue, etc.\n<br>Most columns are in object format(conversion needed), and **key features** include:\n1. **movie itself**: id, title, genres, runtime, adult, original language, keywords, release year...\n2. **rating & popularity**: rating, vote counts, popularity...\n3. **profitability**: revenue, budget, production companies & countries...","metadata":{}},{"cell_type":"markdown","source":"Since the stakeholder of our recommendatoion system is audiences instead of investors, thus our model is not concerned about the profitability, so **columns such as revenue, budgets, should be removed during data preprocessing stage**.","metadata":{}},{"cell_type":"code","source":"# check for any missing values\ndf.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Notied there are significant amount of missing values in columns such as backdrop paths, homepage, poster_path...Since these information are irrelevant to our model building, it is safe to ignore. \n<br>\n<br>However, **release_date** (17% missing) and **genres** (40% missing) are essential to our model development, so **new method should be implemented to solve this issue during data preprocessing stage**.","metadata":{}},{"cell_type":"code","source":"# check for any duplicates\ndf.duplicated().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# relatively small numbers of duplicates, just drop them\ndf.drop_duplicates(inplace=True)\ndf.duplicated().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Content-based Filtering","metadata":{}},{"cell_type":"markdown","source":"Content-based filtering uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback.<br><br>\nIn this case, I will focus on the **intrinsic attributes** of movies in our dataset, which include title, release date, runtime, adult, original language and genres. I will **extract relevant information**, **binary-encode each feature** and **vectorize each data entry**, so that I can **calculate the cosine similarity** between each movie for recommendation.\n","metadata":{}},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"df['title'].isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[~df['title'].isnull()]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['title'].isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert release_date to datetime and extract year\ndf['release_date'] = pd.to_datetime(df['release_date'])\ndf['release_date'].dt.year.value_counts().sort_index()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed some years are wayyyy off the normal timeline, since the first movie ever released is in 1895, and the latest movie in this databse should be releaed in 2024.","metadata":{}},{"cell_type":"code","source":"# view the rows with release years before 1900\ndf[df['release_date'].dt.year < 1900].sort_values('release_date')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # view the rows with release years after 2024\n# df[df['release_date'].dt.year > 2024].sort_values('release_date', ascending=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['release_date'].dt.year > 2025]['status'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# set the reference data as today\nref_date = pd.Timestamp.now()\nref_date\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filter movies that are released in the future\ndf[df['release_date'] > ref_date][['status']].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed movie records with a release year before 1900 are mostly scripts or plays (ex.A Farsa de Inês Pereira), which do not fall into the category under 'movie', thus they should be trimmed off.\n<br><br>Also noticed some movies released after today are marked as 'released'-- they be trimmed off as well, along with any movie that is not under the status of **'released'**.","metadata":{}},{"cell_type":"code","source":"# drop any rows with release year smaller than 1900\ndf = df[df['release_date'].dt.year >= 1900]\n# drop any rows with released data greater than today\ndf = df[df['release_date']<= ref_date]\n# validate\ndf['release_date'].dt.year.value_counts().sort_index()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Any movie that is not under 'released' status is not available to watch, thus should be trimmed off","metadata":{}},{"cell_type":"code","source":"df = df[df['status'] == 'Released']\ndf['status'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Up unil now we still have 974k validate movie data in our databse, it is still a great resort.","metadata":{}},{"cell_type":"code","source":"# check runtime of each movie in the dataset\ndf['runtime'].describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The minumum runtime is 0 and maxium is 14400, something is off.","metadata":{}},{"cell_type":"code","source":"df['runtime'].value_counts().sort_index()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['runtime'] == 0].shape[0]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed there are **208230 records** with 0 runtime, they must be removed.","metadata":{}},{"cell_type":"markdown","source":"Some movies under 15 minutes are shorts (ex.frozen fever), they still fall under the definition of movie, but **they should be categorized as short during data preprocessing stage**.","metadata":{}},{"cell_type":"code","source":"df[df['runtime']>= 360].shape[0]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed there are **1660 records** with runtime greater than 360 minutes (any movie longer than 6 hours is not watchable, in my humble opinion), thus these records need to be trimmed off.","metadata":{}},{"cell_type":"code","source":"df=df[(df['runtime'] != 0) & (df['runtime'] <= 360)]\ndf['runtime'].describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed some movies have zero revenue, even though this attribute is not relevant to our model building, this can be used to validate our data.","metadata":{}},{"cell_type":"code","source":"df[df['revenue'] == 0].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed many blockbuster hits(bird box, Zack Snyder's Justice League) are listed here, so there could be many errors in recording revenue, so we should proceed without trimming.","metadata":{}},{"cell_type":"code","source":"# find movies that have no imdb_id\ndf[df['imdb_id'].isnull()].sample(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most movies without imdb id have very little information about themselves, thus, we will remove these rows.","metadata":{}},{"cell_type":"code","source":"# remove any rows with no imdb_id\ndf = df[df['imdb_id'].notnull()]\ndf.shape[0]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df['adult']==True].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Movies that are marked as 'adult' can be porngraphy, thus they will be removed from movie analysis for user discretion.","metadata":{}},{"cell_type":"code","source":"# remove any rows with adult \ndf = df[df['adult'] == False]\ndf.shape[0]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Text columns include genres, tagline, overviews, and keywords, and they are all important in sub-classify movies based on similarity.","metadata":{}},{"cell_type":"code","source":"df['genres'].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['tagline'].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['overview'].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['keywords'].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed many movies have comprehensive genres and keywords, and different combination can confuse the similarity between each pair, **more data processing is needed**.\n\nColumn 'tagline' and column 'overview' both contain paragraphs that describe movies' plots, they can be subjective -- **remove tagline, and tokenize overview in later preprocessing**.","metadata":{}},{"cell_type":"markdown","source":"What about production companies and languages? Are they comprehensive as well?","metadata":{}},{"cell_type":"code","source":"df['production_countries'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['original_language'].value_counts().head(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed all records only have **one** original language, which can be used in model development, and **total of 10 languages can be major categories during data preprocessing stage.**","metadata":{}},{"cell_type":"code","source":"df['spoken_languages'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed there are incidents with movies have multiple spoken languages, which can means this movie is available in multiple languages.<BR><BR>\nHowever, no guarantee if the audiences will also see similarity between two movies in two different cultures but only available in the same translation , thus **this column will be removed during data preprocessing stage**.","metadata":{}},{"cell_type":"code","source":"df['vote_average'].describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# select samples of movie that has rating over 8.0\ndf_rating = df[df['vote_average'] > 8.0].sample(10, random_state=42)\ndf_rating.sort_values('vote_average', ascending=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed movies with the same rating (vote_average) has very different popularity score, because popularity ranks how popular movies are, and blockbusters may have lower rating, but definitely more popular content for recommendation.","metadata":{}},{"cell_type":"code","source":"df.sort_values('popularity', ascending=False).head(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.sort_values('vote_count', ascending=False).head(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed the most popular movies do not necessarily have more people voting. This is easy to understand as popular movies are usually released not long before the reference date, but movies with most voting tend to be movies that have continually voted for quality check.\n\nBoth can be used to build our model, but voting counts can be redundant as it provides little infotmation.","metadata":{}},{"cell_type":"code","source":"# find the most popular production companies\ndf['production_companies'].value_counts().head(20)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Columns showing a movie's profitability (such as revenue, backdrop_path, budget, homepage) are irrelevant to building our recommendation model, thus they should be removed. <br>\nBut some famous production companies (ex.BBC, Disney) are mentioned, which can be used for model building **with more preprocessing**.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After exploring all columns, some will be removed as they are irrelevant for model building, which include: vote_count(not as informative as rating), status(all'released'), revenue, budget, adult(all are FALSE), backdrop_path, homepage, imdb_id(not informative), original title(repetitive as col 'title'), poster_path, tagline(not as informative as overview), production_countries, spoken_languages(same as original language and difficult to classify).","metadata":{}},{"cell_type":"code","source":"df_for_model = df.copy()\ndf_for_model = df_for_model[['id', 'title', 'runtime','original_language', 'vote_average', 'release_date', 'genres', 'keywords', 'overview', 'production_companies', 'popularity']]\ndf_for_model.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output a csv file for before data preprocessing\n# df_for_model.to_csv('TMDB_movie_dataset_v11_cleaned_for_model.csv', index=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"1. For numerical columns, remains the same.\n2. For caregorical columns with uni-value cell, I will encode categorical features using label encoder; \n3. For caregorical columns with multiple values in one cell (ex.genres), I will parse each value and convert them into usable lists of genres, and encode each distinct genres using multilabel binarizer.\n4. For text columns such as keywords and overviews, I will use tfidf matrix to combine them into one combined text for keyword searchup.\n<br><br>***The transformed table will vectorize each movie with binary encoding.***","metadata":{}},{"cell_type":"code","source":"# df_for_model = pd.read_csv('TMDB_movie_dataset_v11_cleaned_for_model.csv')\n# df_for_model.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check for any missing values\ndf_for_model.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filled missing values with unknown\ndf_for_model['genres'].fillna('unknown', inplace=True)\ndf_for_model['keywords'].fillna('unknown', inplace=True)\ndf_for_model['overview'].fillna('unknown', inplace=True)\ndf_for_model['production_companies'].fillna('unknown', inplace=True)\n# check for any missing values\ndf_for_model.isnull().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extract year from release_date, and refactor it into different time periods based on empirical knowledge\ndef define_era(row):\n    if row['release_date'].year < 1927:\n        return 'The Silent Era'\n    elif row['release_date'].year < 1960:\n        return 'Golden Age'\n    elif row['release_date'].year < 1980:\n        return 'Post-War Era'\n    elif row['release_date'].year < 1990:\n        return 'Blockbuster Era'\n    else:\n        return 'Digital Era'\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure release_date is in datetime format\n# df_for_model['release_date'] = pd.to_datetime(df_for_model['release_date'], errors='coerce')\n\n# Apply the define_era function\ndf_for_model['era'] = df_for_model.apply(define_era, axis=1)\ndf_for_model.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_for_model['era'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# refector runtime into different types of movies based on empirical knowledge\ndef define_runtime(row):\n    if row['runtime'] < 40:\n        return 'Short Film'\n    elif row['runtime'] < 60:\n        return 'Featurette'\n    elif row['runtime'] < 120:\n        return 'Feature Film'\n    elif row['runtime'] < 180:\n        return 'Extended Feature Film'\n    else:\n        return 'Other'","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_for_model['runtime_type'] = df_for_model.apply(define_runtime, axis=1)\ndf_for_model.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_for_model['runtime_type'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\ndf_for_model['original_language'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_for_model['original_language'].value_counts().shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# refector languages into six major langueges and others\ndef define_language(row):\n    if row['original_language']=='en':\n        return 'English'\n    elif row['original_language']=='ja':\n        return 'Japanese'\n    elif row['original_language']=='fr':\n        return 'French'\n    elif row['original_language']=='es':\n        return 'Spanish'\n    elif row['original_language']=='de':\n        return 'German'\n    elif row['original_language']=='it':\n        return 'Italian'\n    else:\n        return 'Other'","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_for_model['language'] = df_for_model.apply(define_language, axis=1)\ndf_for_model.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_for_model['language'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# find all production companies listed in the dataset\nproduction_companies_list = df_for_model['production_companies'].value_counts().index.tolist()\nproduction_companies_list[:50]\n# split each element in the list by comma\nproduction_companies_list = [i.split(', ') for i in production_companies_list]\n# flatten the list\nproduction_companies_list = [item for sublist in production_companies_list for item in sublist]\n# remove duplicates\nproduction_companies_list = list(set(production_companies_list))\n# remove empty strings\nproduction_companies_list = [i for i in production_companies_list if i]\n# remove unknown\nproduction_companies_list = [i for i in production_companies_list if i != 'unknown']\nlen(production_companies_list)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Too many production companies, choose only major ones.","metadata":{}},{"cell_type":"code","source":"major_production_companies = df_for_model['production_companies'].value_counts().head(20).index.tolist()\nmajor_production_companies","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def define_production_company(row):\n    for company in major_production_companies:\n        if company in row['production_companies']:\n            return company\n    return 'Other'","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_for_model['production_company'] = df_for_model.apply(define_production_company, axis=1)\ndf_for_model.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_for_model['production_company'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# use dummy variables to encode era, runtime_type, language, and production company\ndf_for_model = pd.get_dummies(df_for_model, columns=['era', 'runtime_type', 'language', 'production_company'])\n\n# oh = OneHotEncoder(handle_unknown='ignore')\n# oh.fit(df_for_model[['era', 'runtime_type', 'original_language', 'production_company']])\n# # transform the categorical variables into one-hot encoded variables\n\n# df_for_model['era'] = oh.transform(df_for_model['era'])\n# df_for_model['runtime_type'] = oh.transform(df_for_model['runtime_type'])\n# df_for_model['original_language'] = oh.transform(df_for_model['original_language'])\n# df_for_model['production_company'] = oh.transform(df_for_model['production_company'])\npd.set_option('display.max_columns', None)\ndf_for_model.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# make a list of genres in each row\ngenre_l = df_for_model['genres'].apply(lambda x: x.split(', ')).reset_index(drop=True)\ntype(genre_l)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed multiple genres are associated with the same movie-need to filter out unique genres by iterating each sub-list of genres of each movie.","metadata":{}},{"cell_type":"code","source":"# find all genres in genre_l\ngenre_l = genre_l.to_list()\ngen_lst = []\nfor i in range(len(genre_l)):\n    for j in range(len(genre_l[i])):\n        gen_lst.append(genre_l[i][j])\ngen_lst = pd.Series(gen_lst)\ngen_lst.value_counts()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Noticed there are only 20 genres in total(including unknown), we can encode them all using **multilabel binarizer**.","metadata":{}},{"cell_type":"code","source":"# encode genres using MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\ngen_l = pd.Series(genre_l)\ngenre_encoded = mlb.fit_transform(genre_l)\ngenre_encoded[:5]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_genre_encoded = pd.DataFrame(genre_encoded, columns=mlb.classes_)\ndf_genre_encoded","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure 'genres' column is present before dropping it\nif 'genres' in df_for_model.columns:\n\tdf_for_model.drop('genres', axis=1, inplace=True)\ndf_for_model_encoded = pd.concat([df_for_model.reset_index(drop=True), df_genre_encoded.reset_index(drop=True)], axis=1)\npd.set_option('display.max_columns', None)\ndf_for_model_encoded.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It seems column overview has little information regarding the overall context compared to keywords, will remove for computational efficiency.","metadata":{}},{"cell_type":"code","source":"# remove unnecessary  columns\ndf_for_model_encoded.drop(['id', 'runtime', 'original_language', 'release_date', 'overview', 'production_companies'], axis=1, inplace=True)\ndf_for_model_encoded.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Text columns preprocessing","metadata":{}},{"cell_type":"code","source":"# put all distinct keywords into a giant list\n\nkeyword_l = list(set(keyword for sublist in df_for_model['keywords'].apply(lambda x: x.split(', ')).reset_index(drop=True) for keyword in sublist))\nlen(keyword_l)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keyword_l[:10]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show all texts in the column\n# pd.set_option('display.max_colwidth', None)\n# df_for_model[['title', 'keywords', 'overview']].head(10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### NLP on Text Columns using word embedding, PCA, and K-Means","metadata":{}},{"cell_type":"markdown","source":"**Preprocessing**: Clean and preprocess text columns including lowercasing, removing punctustion, removing stop words(filler words), lemmatizing(combining synonyms), and combining keywords and overview into one column 'combine_text'.","metadata":{}},{"cell_type":"code","source":"# Convert all text columns to lowercase and remove punctuation\nimport re\npunctuation = \"!\\\"#$%&'()*+-./:;<=>?@[\\]^_`{|}~\"\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(re.escape(punctuation), '', text)  # Remove punctuation\n    return text\ndf_for_model_encoded['keywords'] = df_for_model_encoded['keywords'].apply(clean_text)\ndf_for_model_encoded.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# remove duplicates words in keywords and overview\ndef remove_duplicates(text):\n    words = text.split()\n    unique_words = list(dict.fromkeys(words))\n    return ' '.join(unique_words)\ndf_for_model_encoded['keywords'] = df_for_model_encoded['keywords'].apply(remove_duplicates)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# remove stop words from overview\nstop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"would\", \"yet\"]\ndf_for_model_encoded['keywords'] = df_for_model_encoded['keywords'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# use WordNetLemmatizer to lemmatize the overview\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_text(text):\n    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\ndf_for_model_encoded['keywords'] = df_for_model_encoded['keywords'].apply(lemmatize_text)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"keyword_l = list(set(keyword for sublist in df_for_model['keywords'].apply(lambda x: x.split(', ')).reset_index(drop=True) for keyword in sublist))\nlen(keyword_l)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# overview_l = list(set(keyword for sublist in df_for_model['overview'].apply(lambda x: x.split(', ')).reset_index(drop=True) for keyword in sublist))\n# len(overview_l)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# combine keywords & overview into one column named combine_text, then use tfidf to vectorize the text\n# df_for_model_encoded['combine_text'] = df_for_model_encoded['keywords'] + ' ' + df_for_model_encoded['overview']\n# df_for_model_encoded['combine_text'].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Word Embedding using TF IDF","metadata":{}},{"cell_type":"markdown","source":"**Vectorization**: Convert words into numerical representations using word embedding","metadata":{}},{"cell_type":"markdown","source":"**FastText** and **TF-IDF** are both methods used in Natural Language Processing (NLP) for representing text, but they differ significantly in their approach and capabilities. FastText is a word embedding technique that learns vector representations of words **based on their character n-grams**, while TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure that **reflects a word's importance within a document relative to a collection of documents**. <br>\nIn this case, it is important to find the most frequent words and compare their similarity within the corpus of all cleaned texts given in the dataset, and TF-IDF is a better tool for word embedding.","metadata":{}},{"cell_type":"code","source":"# use tfidf to vectorize the text\ntfidf = TfidfVectorizer(max_features=1000, token_pattern=r'[^,]+')\ntfidf_matrix = tfidf.fit_transform(df_for_model_encoded['keywords'])\ntfidf_matrix.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert the sparse matrix to a dense matrix\ntfidf_matrix_dense = tfidf_matrix.todense()\n# convert the dense matrix to a dataframe\ndf_tfidf = pd.DataFrame(tfidf_matrix_dense, columns=tfidf.get_feature_names_out())\ndf_tfidf.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output df_tfidf to csv\ndf_tfidf.to_csv('keyword_embedding.csv', index=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print out all columns name in df_tfidf\nkeywords_searchup = df_tfidf.columns.tolist()\n# output the keywords_searchup to a txt file\nwith open('keywords_searchup.txt', 'w') as f:\n    for item in keywords_searchup:\n        f.write(\"%s\\n\" % item)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# use PCA to reduce the dimensionality of the tfidf matrix\n# set the number of components to 90% of the variance (explaine the variance)\npca = PCA(n_components=0.90)\n# convert the dense matrix to a numpy array\ntfidf_matrix_dense = np.array(tfidf_matrix_dense)\n# fit the PCA model to the dense matrix\npca.fit(tfidf_matrix_dense)\npca_matrix = pca.transform(tfidf_matrix_dense)\npca_matrix.shape\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop the keyword columns from df_for_model_encoded, then output the csv file\ndf_for_model_encoded.drop(['keywords'], axis=1, inplace=True)\ndf_for_model_encoded.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# concat the pca matrix with the original dataframe\ndf_pca = pd.DataFrame(pca_matrix, columns=[f'pca_{i}' for i in range(pca_matrix.shape[1])])\ndf_pca.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# concat the pca matrix with the original dataframe for similarity search\ndf_for_model_encoded_sim = pd.concat([df_for_model_encoded.reset_index(drop=True), df_pca.reset_index(drop=True)], axis=1)\n# df_for_model_encoded_sim.drop('keywords', axis=1, inplace=True)\ndf_for_model_encoded_sim.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# combine the tfidf matrix with the original dataframe\n# df_for_model_encoded = pd.concat([df_for_model_encoded.reset_index(drop=True), df_tfidf.reset_index(drop=True)], axis=1)\n# df_for_model_encoded.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from gensim.models import Word2Vec","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create a CBOW Word2Vec model \n# model1 = Word2Vec(tokenized_words, vector_size=200, window=5, min_count=1, workers=4)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # test the model on example keywords, and find the most similar words by cosine similarity\n# model1.wv.most_similar('rescue')[:10]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create a Skip Gram Word2Vec model \n# model2 = Word2Vec(tokenized_words, vector_size=200, window=10, min_count=1, workers=4, sg=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test the model on example keywords, and find the most similar words by cosine similarity\n# model2.wv.most_similar('rescue')[:10]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from gensim.models import FastText","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model3 = FastText(tokenized_words, vector_size=200, window=5, min_count=1, workers=4)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model3.wv.most_similar('love')[:10]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# word_vectors = [model3.wv[word] for word in processed_words if word in model3.wv]\n# dense_matrix = np.array(word_vectors)\n# dense_matrix.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# no_vec_lst = [word for word in processed_words if word not in model3.wv]\n# len(no_vec_lst)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.decomposition import PCA\n# pca = PCA(n_components=10)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reduced_word_vectors = pca.fit(dense_matrix).transform(dense_matrix)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check whether dimensions have been reduced to 50\n# len(reduced_word_vectors[0])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# len(reduced_word_vectors)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import  the necessary libraries for clustering\n# from sklearn.cluster import KMeans","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#find the optimal k using the elbow method\n# wcss = []\n# for k in range(1, 20):\n#     kmeans = KMeans(n_clusters=k, random_state=42)\n#     kmeans.fit(reduced_word_vectors)\n#     wcss.append(kmeans.inertia_)\n# plt.plot(range(1, 20), wcss)\n# plt.xlabel('Number of clusters')\n# plt.ylabel('WCSS')\n# plt.title('Elbow Method')\n# plt.show()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# k = 10\n# kmeans = KMeans(n_clusters=k, random_state=42)\n# kmeans.fit(reduced_word_vectors)\n# labels = kmeans.labels_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluation**: use **k-means inertia** to measure the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster","metadata":{}},{"cell_type":"code","source":"# find out the intertia of the CURRENT kmeans model\n# kmeans.inertia_","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A good k-means model is one with low inertia AND a low number of clusters ( K ). Since the inertia is very close to 0(0.06), we will consider we have a good k-means clustering and the 10 labels can successfully group all processed keywords.","metadata":{}},{"cell_type":"markdown","source":"crete a dataframe to apply each row of the tokenized words to the map_to_word_vector function","metadata":{}},{"cell_type":"code","source":"# tokenized_words_series = pd.Series(tokenized_words)\n# df_tokenized_words = pd.DataFrame(tokenized_words_series, columns=['tokenized_words'])\n# df_tokenized_words.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# map each tokenized word to its corresponding word vectors, if it is not in the word vector matrix, then use a zero vector\n# def map_to_word_vector(row):\n#     word_vector = np.zeros(200)\n#     for word in row:\n#         if word in model3.wv:\n#             word_vector += model3.wv[word]\n#     return word_vector","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_tokenized_words['word_vector'] = df_tokenized_words['tokenized_words'].apply(map_to_word_vector)\n# df_tokenized_words.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# make sure each tokenized word find its corresponding word vector\n# df_tokenized_words.isna().sum()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_tokenized_words.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lables_series = pd.Series(labels)\n# lables_series.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_tokenized_words['label'] = lables_series\n# df_tokenized_words.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check if each cluster has appropraiet number of words\n# df_tokenized_words['label'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_tokenized_words[df_tokenized_words['label']==0].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"There are some form of similarities between the semantic relationship between words in each value, so for now we can proceed.","metadata":{}},{"cell_type":"code","source":"# transform dataframe by making label as a dummy variable\n# df_tokenized_words_getdummies = pd.get_dummies(df_tokenized_words, columns=['label'])\n# df_tokenized_words_getdummies.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load the similarity searchup dataset\ndf_for_model_encoded_sim = pd.read_csv('TMDB_movie_dataset_v11_cleaned_for_model_encoded_sim.csv')\ndf_for_model_encoded_sim.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# set title as index\ndf_for_model_encoded_sim.set_index('title', inplace=True)\n# check the index\ndf_for_model_encoded_sim.index[:10]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop any null values\ndf_for_model_encoded_sim.dropna(inplace=True)\ndf_for_model_encoded_sim.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop any duplicates\ndf_for_model_encoded_sim.drop_duplicates(inplace=True)\ndf_for_model_encoded_sim.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# find the highest popularity score\ndf_for_model_encoded_sim['popularity'].describe()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Assign weighting","metadata":{}},{"cell_type":"markdown","source":"Before we proceed, we need to assign weights to different columns so that:\n1. For numeric features, standardize them to the scale of 1 by min_max or standardscaler.\n2. For binary variable, since each movie can only take on one label, no need for further scaling.\n3. For multi-labeled features (genres) where each movie can have rating in different dummy variables of the same features would sum up to 1 <br>\n(ex.if movie 1 has 2 in genre 1, 3 in genre 2, and 5 in genre 3, <br>\nthen each value will be converted to 2/(2+3+5)=0.2 in genre 1, 3/(2+3+5)=0.3 in genre 2, and 5/(2+3+5)=0.5 in genre 3)\n4. Keywords columns are already transformed, leave for now.","metadata":{}},{"cell_type":"code","source":"df_for_model_encoded_sim.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# standardize numerical features\nscaler = StandardScaler()\ndf_for_model_encoded_sim[['vote_average', 'popularity']] = scaler.fit_transform(df_for_model_encoded_sim[[ 'vote_average', 'popularity']])\ndf_for_model_encoded_sim.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"era_columns = [col for col in df_for_model_encoded_sim.columns if 'era_' in col]\nruntime_type_columns = [col for col in df_for_model_encoded_sim.columns if 'runtime_type_' in col]\nlanguage_columns = [col for col in df_for_model_encoded_sim.columns if 'language_' in col]\nproduction_company_columns = [col for col in df_for_model_encoded_sim.columns if 'production_company_' in col]\n# check the columns\nprint(era_columns)\nprint(runtime_type_columns)\nprint(language_columns)\nprint(production_company_columns)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For binary columns, make sure all values are converted to numeric.","metadata":{}},{"cell_type":"code","source":"# convert all binary columns to numeric\n# df_for_model_encoded_sim[era_columns] = df_for_model_encoded_sim[era_columns].apply(pd.to_numeric)\n# df_for_model_encoded_sim[runtime_type_columns] = df_for_model_encoded_sim[runtime_type_columns].apply(pd.to_numeric)\n# df_for_model_encoded_sim[language_columns] = df_for_model_encoded_sim[language_columns].apply(pd.to_numeric)\n# df_for_model_encoded_sim[production_company_columns] = df_for_model_encoded_sim[production_company_columns].apply(pd.to_numeric)\n# check the columns\ndf_for_model_encoded_sim.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# relabel genre columns so that rating in different genres would sum up to 1 \ngenre_columns = ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', 'TV Movie', 'Thriller', 'War', 'Western', 'unknown']\ndf_for_model_encoded_sim[genre_columns] = df_for_model_encoded_sim[genre_columns].div(df_for_model_encoded_sim[genre_columns].sum(axis=1), axis=0)\ndf_for_model_encoded_sim.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # relabel keywords so that rating in different labels would sum up to 1 \n# # (ex.if movie 1 has 2 in label 1, 3 in label 2, and 5 in label 3, then each value will be converted to 2/(2+3+5)=0.2 in label 1, 3/(2+3+5)=0.3 in label 2, and 5/(2+3+5)=0.5 in label 3)\n# keyword_columns = [f'label_{i}' for i in range(10)]\n# df_for_model_encoded_final[label_columns] = df_for_model_encoded_final[label_columns].div(df_for_model_encoded_final[label_columns].sum(axis=1), axis=0)\n# df_for_model_encoded_final.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Determining the appropriate weights for different features in a movie dataset involves careful consideration of how much each feature contributes to the overall similarity measure. <br><br>\nHere are the proposed weighting:\n\n1. **genres & languages: 4** -- Genres are often the primary factor in movie selection for viewers. They define the storytelling style, themes, and overall expectations of the film, making this a crucial component in determining film similarity and preferences. Viewers also tend to watch movies in their native languages (unless foreign films are specified), so it would make more sense to prioritize langues as well.\n2. **rating & popularity: 3** -- This feature could be highly influential in determining quality of the movies. Most viewers are looking for recommendation that would .\n3. **runtime & era: 2** -- Runtime can influence a viewer’s choice (e.g., a preference for short films for casual viewing versus feature films for dedicated watching). However, it could be secondary to the content and thematic similarity as a feature in most analyses. Era can help contextualize a film's style, themes, and production values. Movies from the same era might share stylistic features, making this a relevant feature. However, like runtime type, it shouldn’t be as heavily weighted as genres.\n4. **keywords: 1** -- Keywords can capture detailed thematic elements and narrative aspects that go beyond simple classifications. They, however, are highly subjective in evaluating movies' similarity, especially when genres are present, as a result, it is secondary in weighting.\n5. **production company: 0.8** -- This usually works for viewers who have strong preferences for certain production (ex.Disney), so they are less important.\n","metadata":{}},{"cell_type":"code","source":"df_for_model_encoded_sim[genre_columns] = df_for_model_encoded_sim[genre_columns]*4\ndf_for_model_encoded_sim[language_columns] = df_for_model_encoded_sim[language_columns]*4\ndf_for_model_encoded_sim['vote_average'] = df_for_model_encoded_sim['vote_average']*3\ndf_for_model_encoded_sim['popularity'] = df_for_model_encoded_sim['popularity']*3\ndf_for_model_encoded_sim[era_columns] = df_for_model_encoded_sim[era_columns]*2\ndf_for_model_encoded_sim[runtime_type_columns] = df_for_model_encoded_sim[runtime_type_columns]*2\ndf_for_model_encoded_sim[production_company_columns] = df_for_model_encoded_sim[production_company_columns]*0.8\ndf_for_model_encoded_sim.head()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# era_columns = ['era_Digital Era', 'era_Blockbuster Era', 'era_Golden Age', 'era_Post-War Era', 'era_The Silent Era']\n# df_for_model_encoded_final[era_columns] = df_for_model_encoded_final[era_columns]*2\n# df_for_model_encoded_final.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# runtime_columns = ['runtime_type_Epic Length Film', 'runtime_type_Extended Feature Film', 'runtime_type_Feature Film', 'runtime_type_Featurette', 'runtime_type_Short Film']\n# df_for_model_encoded_final[runtime_columns] = df_for_model_encoded_final[runtime_columns]*2\n# df_for_model_encoded_final.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_for_model_encoded_final[genre_columns] = df_for_model_encoded_final[genre_columns]*4\n# df_for_model_encoded_final.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_for_model_encoded_norm = scaler.fit_transform(df_for_model_encoded_final.drop('title',axis=1))\n# df_norm_df = pd.DataFrame(df_for_model_encoded_norm, columns=[x for x in df_for_model_encoded_final.columns if x not in 'title'])\n# df_norm_df.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_final = pd.concat([df_for_model_encoded_final['title'], df_norm_df], axis=1)\n# df_final.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_final.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# set movie title as index\n# df_final.set_index('title', inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output the final dataframe to a csv file for a later use\n# df_final.to_csv('movie_rec_databse.csv')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output the final dataframe to a csv file for a later use\ndf_for_model_encoded_sim.to_csv('movie_rec_databse_2.csv')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Deployment","metadata":{}},{"cell_type":"markdown","source":"### Cosine Similarity","metadata":{}},{"cell_type":"markdown","source":"In content-based filtering, the most common similarity comparison methods used to determine how similar two items are based on their features are cosine similarity, Euclidean distance, Jaccard similarity. <br><br>\nHere we will choose **cosine similarity** because of its ability to handle sparse data and high-dimensional feature spaces effectively. ","metadata":{}},{"cell_type":"code","source":"# return the top 10 most similar movies from the original dataframe, given their cosine similarity\ndef get_recommendation():\n\n    movie_name = input(\"Enter the movie name you are looking for: \").strip().lower().replace(' ', '')\n    \n    # make sure n is a valid integer between 1 and 20\n    while True:\n        n_input = input(\"How many movies are you looking for to recommend? (default is 10): \").strip()\n        if n_input == \"\":\n            n = 10\n            break\n        try:\n            n = int(n_input)\n            if n < 1 or n > 20:\n                print(\"Please choose a number between 1 and 20.\")\n            else:\n                break\n        except ValueError:\n            print(\"Please enter a valid integer.\")\n\n    # load the encoded dataframe for cosine similarity calculation\n    df_final = pd.read_csv('/kaggle/input/movie-recommendation-title-for-similarity-csv/movie_rec_databse_2.csv')\n    df_final.set_index('title', inplace=True)\n    # Standardize movie names in the final dataframe\n    df_final.index = df_final.index.str.strip().str.lower().str.replace(' ', '')\n    \n    # load the original dataset\n    df = pd.read_csv('/kaggle/input/tmdb-movies-dataset-2023-930k-movies/TMDB_movie_dataset_v11.csv')\n    # drop duplicate values\n    df.drop_duplicates(inplace=True)\n    # drop null values\n    df.dropna(inplace=True)\n    # standardize movie names in the original dataset\n    df['title'] = df['title'].str.strip().str.lower().str.replace(' ', '')\n    \n    if movie_name not in df_final.index:\n        print(f\"No match is available yet. Here are the top {n} trending movies for inspiration:\")\n        trending_movies = df_final.head(n)\n        return trending_movies\n\n    new_df = df_final.loc[[movie_name]]\n    # Remove rows with NaN values\n    df_other = df_final.loc[df_final.index != movie_name, :].dropna()\n    # Get the titles of the other movies\n    df_titles = df_other.index\n    cosine_sim_matrix = cosine_similarity(new_df, df_other)\n    cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=[movie_name], columns=df_titles)\n    # Get the top n most similar movies\n    top_n_similar = cosine_sim_df.T.sort_values(by=movie_name, ascending=False).head(n)\n    # Slice out movie‘s information from the original dataset by title\n    top_n_similar_df = df.loc[df['title'].isin(top_n_similar.index)]\n    return top_n_similar_df\n\n# Example usage:\nget_recommendation()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:23:24.351241Z","iopub.execute_input":"2025-04-11T03:23:24.351652Z","iopub.status.idle":"2025-04-11T03:25:33.381327Z","shell.execute_reply.started":"2025-04-11T03:23:24.351624Z","shell.execute_reply":"2025-04-11T03:25:33.379657Z"}},"outputs":[],"execution_count":null}]}